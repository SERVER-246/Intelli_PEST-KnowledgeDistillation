# Knowledge Distillation for Pest Classification

Multi-teacher knowledge distillation pipeline to create a lightweight, efficient model for pest classification using 11 pre-trained teacher models.

## ğŸ“ Project Structure

```
KnowledgeDistillation/
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ config.yaml          # Main configuration file
â”‚   â””â”€â”€ class_mapping.json   # Class name to index mapping
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ student_model.py     # Lightweight CNN architecture
â”‚   â”œâ”€â”€ dataset.py           # Data loading and augmentation
â”‚   â”œâ”€â”€ trainer.py           # Knowledge distillation trainer
â”‚   â””â”€â”€ exporter.py          # Model export utilities
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ student/             # Student model checkpoints
â”‚   â””â”€â”€ exports/
â”‚       â”œâ”€â”€ pytorch/         # PyTorch exports (.pt)
â”‚       â”œâ”€â”€ onnx/            # ONNX exports (.onnx)
â”‚       â””â”€â”€ tflite/          # TFLite exports (.tflite)
â”œâ”€â”€ checkpoints/             # Training checkpoints
â”œâ”€â”€ logs/                    # Training logs
â”œâ”€â”€ metrics/                 # Training metrics (JSON)
â”œâ”€â”€ train.py                 # Main training script
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸš€ Quick Start

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

### 2. Configure Training

Edit `configs/config.yaml` to set:
- Dataset path
- Training hyperparameters
- Teacher model locations
- Export settings

### 3. Run Training

```bash
# Use default config
python train.py

# Override settings
python train.py --data_dir "path/to/dataset" --epochs 100 --batch_size 32

# Use smaller model for mobile
python train.py --model_type small

# Resume training
python train.py --resume checkpoints/latest_checkpoint.pt
```

### 4. Export Only

```bash
python train.py --export_only checkpoints/best_checkpoint.pt
```

## ğŸ“Š Dataset Format

The dataset should be organized in ImageFolder format:
```
IMAGE DATASET/
â”œâ”€â”€ class_1/
â”‚   â”œâ”€â”€ image1.jpg
â”‚   â”œâ”€â”€ image2.jpg
â”‚   â””â”€â”€ ...
â”œâ”€â”€ class_2/
â”‚   â””â”€â”€ ...
â””â”€â”€ ...
```

## ğŸ“ Teacher Models

Uses 11 pre-trained ONNX models:
- **Base models**: MobileNetV2, ResNet50, InceptionV3, EfficientNet-B0, DarkNet53, AlexNet, YOLO11n-cls
- **Ensemble models**: Attention, Concat, Cross, Super Ensemble

## ğŸ“ˆ Training Features

- **Knowledge Distillation**: Soft labels from teacher ensemble
- **Temperature Scaling**: Soften probability distributions
- **Weighted Ensemble**: Higher weight for ensemble teachers
- **Mixed Precision**: FP16 training for faster computation
- **Data Augmentation**: Rotation, flip, color jitter, etc.
- **Weighted Sampling**: Handle class imbalance
- **Early Stopping**: Prevent overfitting
- **Comprehensive Logging**: Track all metrics

## ğŸ”§ Student Model Architecture

Custom lightweight CNN with:
- Depthwise separable convolutions
- Squeeze-and-Excitation blocks
- Residual connections
- Global average pooling

**Model Variants:**
| Variant  | Size  | Parameters |
|----------|-------|------------|
| Standard | ~15MB | ~4M        |
| Small    | ~8MB  | ~2M        |
| Tiny     | ~3MB  | ~800K      |

## ğŸ“¦ Export Formats

- **PyTorch** (.pt): Full model with weights
- **ONNX** (.onnx): Opset 13 for cross-platform
- **TFLite** (.tflite): TF 2.14 compatible for Android

## ğŸ“‹ Metrics Tracked

- Training/Validation Loss
- Accuracy (overall and per-class)
- Precision, Recall, F1 Score
- Confusion Matrix
- Learning Rate

## ğŸ“„ Output Files

After training:
- `checkpoints/best_checkpoint.pt` - Best model
- `metrics/training_history.json` - Full training history
- `models/exports/` - Exported models
- `logs/` - Training logs
- `training_summary.json` - Final summary

## ğŸ”¬ Configuration Options

See `configs/config.yaml` for all options:

```yaml
distillation:
  temperature: 4.0    # Softening temperature
  alpha: 0.7          # Weight for soft labels
  beta: 0.3           # Weight for hard labels

training:
  epochs: 100
  learning_rate: 0.001
  optimizer: adamw
  scheduler: cosine
```

## ğŸ“ License

MIT License

## ğŸ‘¥ Authors

Knowledge Distillation Pipeline - December 2024
